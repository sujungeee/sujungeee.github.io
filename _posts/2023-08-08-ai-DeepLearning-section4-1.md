---
title: "[딥러닝] 섹션4-1: RNN 개념"
author: sujungeee
date: 2023-08-08 02:09:00 +0800
categories: [인공지능, 딥러닝]
tags: [인공지능, AI, 딥러닝, 머신러닝, TensorFlow, RNN]
render_with_liquid: false

---



※ 본 포스팅은 인프런 "차량 번호판 인식 프로젝트와 TensorFlow로 배우는 딥러닝 영상인식 올인원" 을 참고하여 작성하였습니다.



#### ✅  순환신경망(RNN)

- RNN(Recurrent Neural Networks)

  : 자연어 처리, 시계열 데이터를 다루는 인공 신경망

  - 시계열 데이터: 시간 축을 중심으로 현재 시간의 데이터가 앞, 뒤 시간의 데이터와 연관 관계를 가지고 있는 데이터
  - 대표적인 시계열 데이터: 주식 가격, 파형으로 표현되는 음성 데이터, 앞뒤 문맥을 가진 단어들의 집합으로 표현되는 자연어 데이터

- RNN 의 구조

  : ANN + 순환 구조

  → (t) 시간대의 은닉층의 출력 값을 (t+1) 시간대의 은닉층의 입력 값으로 씀

  - 은닉층의 출력 값

    - x_t : 시간 *t*에서의 입력 벡터 (input at time *t*)
    - h_t : 시간 *t*에서의 은닉 상태 벡터 (hidden state at time *t*)
    - W_hx : 입력에서 은닉 상태로의 가중치 행렬
    - W_hh : 이전 은닉 상태에서 현재 은닉 상태로의 가중치 행렬
    - b_h : 편향 벡터

    h_t=activation(W_hx⋅x_t+W_hh⋅h_t−1+b_h)

    > 시간축의 처음, 즉 t=0 일 때 은닉층의 출력 값은 임의로 지정
    >
    > : 보통 0 으로 초기화함

  - 출력층의 출력 값

    : ANN 의 출력 값과 동일

- RNN 의 특징

  - 이전 상태에 대한 정보를 일종의 메모리 형태로 저장할 수 있음
  - 시계열 데이터를 다루기에 적절

- Unfolded Representation

  : 예를 들어, 5개의 단어로 이루어진 문장에서 순환 5번을 진행할 때

  → 순환 연결이 없도록 표현한다면 5층 연속으로 인공신경망을 쌓는 구조가 됨



#### ✅  경사도 사라짐 문제(Vanishing Gradient Problem) & LSTM & GRU

- 경사도 사라짐 문제

  : 입력층, 은닉층, 출력층의 구조에서 출력층의 에러 값이 back propagation 에 의해 앞으로 넘어올 때 사라지는 현상

  - 발생 원인

    - 딥러닝 구조에서는 은닉층을 깊게 쌓는 경향이 있기 때문에 해당 문제가 발생하게 됨

    - **RNN 구조에서는 시간 축을 따라 지속적으로 시계열 데이터를 받기 때문에 해당 문제가 발생하게 됨**

      : 초반 시간의 은닉층의 출력 데이터 특성이 시간이 흐르면서 옅어짐(새로운 입력 데이터가 계속 들어오기 때문)

  - 문제점

    : RNN 은 장기 기억력을 가지지 못함 → LSTM 제안

- LSTM(장/단기 기억 네트워크- Long-Short Term Memory Networks)

  : 경사도 사라짐 문제를 해결하기 위해서 제안된 발전된 RNN 구조

  - LSTM 의 구조

    - 메모리 블럭: 인풋 게이트(Input Gate), 포겟 게이트(Forget Gate), 아웃풋 게이트(Output Gate) 의 세 게이트를 총칭
    - 하나의 은닉층을 위의 세 게이트로 대체

  - LSTM 으로 경사도 사라짐 문제를 해결하는 법

    : 인풋, 포겟, 아웃풋 게이트를 열고 닫으면서 영향력을 조절

    ⇒ 장기 기억력: 시간 2-6 의 인풋 게이트를 닫으면 시간 1의 영향력을 오래 가져갈 수 있음

    - Input Gate: 해당 시간 축의 입력 노드에서 넘어온 데이터의 영향력을 받을지 말지를 결정
    - Forget Gate: 이전 시간 축에서 넘어온 은닉층의 영향력을 받을지 말지를 결정
    - Output Gate: 해당 시간 축에서 은닉층의 출력 값을 출력 노드로 반영할지 말지를 결정

  - 메모리 블럭 연산

    : 수식은 pdf 파일 참고

    - 인풋 게이트에서 일어나는 연산

    - 포겟 게이트에서 일어나는 연산

    - 블럭 인풋 게이트에서 일어나는 연산

      > 블럭 인풋 게이트: 메모리 블럭의 입력 값에 활성 함수를 씌워주는 게이트

    - **셀에서 일어나는 연산**

      : 인풋 게이트와 포겟 게이트를 열고 닫아서 현재 시간과 이전 시간의 데이터의 영향력을 가져갈지 혹은 잊어버릴지를 결정하게 됨

    ⇒ 인풋 게이트와 포겟 게이트의 출력 값은 0~1 사이로, 최적화 과정으로 알맞은 값을 찾아가게 됨

    - 아웃풋 게이트에서 일어나는 연산

      1. 아웃풋 게이트의 출력 값(활성 함수를 적용, (b_0)^t)

      2. 현재 시간 t 의 메모리 블럭에서 방출하는 값((b_c)^t)

         : 해당 값을 구할 때 1번이 coefficient 가 되어 메모리 블럭에서 방출하는 값을 구할 수 있으며,

         이는 곧 아웃풋 게이트의 열고 닫힘을 컨트롤할 수 있음

- GRU(Gate Recurrent Unit)

  : LSTM 의 간략화 버전

  - GRU 의 구조
    - 리셋 게이트(Reset Gate): 이전 시간 축의 영향력을 결정(포겟 게이트와 비슷한 역할)
    - 업데이트 게이트(Update Gate): 인풋 게이트의 역할과 유사
  - GRU 의 특징
    - LSTM 이 더 강력한 성능을 보여주지만 대체적으로 비슷한 성능
    - 컴퓨팅 환경이 좋지 못한 경우 LSTM 대신 GRU 를 사용해도 좋음



#### ✅  임베딩(Embedding)의 개념 & Char-RNN

> Sparse 한 데이터의 문제점
>
> 1. 딥러닝 알고리즘이 특징을 충분히 학습할 수 없음
> 2. 낭비되는 표현력이 많아짐

- 임베딩(Embedding)

  : Sparse 한 One-hot Encoding 의 데이터 표현을 Dense 한 표현 형태로 변환하는 기법

  → 자연어 처리 문제를 다룰 때 널리 사용되는 기법

  → 원본 데이터에 dense 한 임베딩 행렬을 곱하여 데이터의 표현 형태를 변환

  - 임베딩의 장점

    - Sparse → Dense: 딥러닝 알고리즘을 학습하기에 적합한 형태가 됨

    - Dimension Reduction: 10000 → 300, 연산량이 감소

    - 만약 임베딩 행렬이 잘 학습됐다면

      → 곱한 결과가 유의미한 단어들간의 연관성을 표현할 수 있게 됨

> Language Modeling: 컴퓨터로 자연어를 처리할 때 단어들의 배열에 기반하여 다음에 어떤 단어가 오는 것이 적합한지 예측하는 모델
>
> → 자연어 처리 문제에서 광범위하게 사용되는 개념

- Char-RNN

  : Language Modeling 을 글자 단위로 예측

  - Char-RNN 의 구성

    1. 알파벳 26 자의 One-hot Encoding 을 26차원으로 임베딩

       (임의로 선택된 첫 번째 인풋 데이터를 임베딩을 통해 변환)

    2. RNN Layer 로 학습 진행

    3. Softmax Regression 으로 26개 알파벳의 확신의 정도를 구하여 argmax 를 통해 가장 확률이 높은 문자를 정답으로 예측

    4. 예측한 문자를 다시 임베딩하여 RNN Layer 로 학습(반복)



#### ✅  Gradient Clipping

- Exploding Gradient Problem

  : Gradient 가 발산하는 현상, RNN 에서 발생하기 쉬운 문제

- Gradient Clipping

  : Exploding Gradient Problem 의 해결책

  - 진행 과정
    - grad_clip 값을 임의 설정
    - gradient 값이 설정한 grad_clip 을 넘는다면 grad_clip 을 gradient 로 나눠줌
    - 나눠준 값(clipped_grads)을 최적화 연산에 활용
