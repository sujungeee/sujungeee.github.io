---
title: "[딥러닝] 섹션1-1: 머신러닝/딥러닝 기초"
author: sujungeee
date: 2023-07-15 17:12:00 +0800
categories: [AI, 딥러닝]
tags: [AI, 딥러닝, 머신러닝, TensorFlow]
render_with_liquid: false

---



※ 본 포스팅은 인프런 "차량 번호판 인식 프로젝트와 TensorFlow로 배우는 딥러닝 영상인식 올인원" 을 참고하여 작성하였습니다.



#### ✅  딥러닝/텐서플로 응용 분야

- 딥러닝 알고리즘의 주요 응용 분야

  - **Computer Vision**: 컴퓨터가 인간의 시각 기능을 수행할 수 있도록 하는 방법을 연구하는 분야

    - 컴퓨터 비전의 대표적인 문제들: Image Classification, Semantic Image Segmentation, Object Detection

    - 컴퓨터 비전 문제를 풀기 위해선 딥러닝의 여러 구조 중 CNN 이 많이 사용됨

    - 컴퓨터 비전의 응용 사례: Tesla Autopilot

      ⇒ 테슬라 자동차는 8개의 카메라와 12개의 초음파 센서를 통해 주변 물체를 감지

      ⇒ 또한 자체 제작한 하드웨어를 통해 빠른 딥러닝 연산을 수행

    - Autopilot: AI 와 컴퓨터 비전 기술을 이용해서 자율 주행을 수행할 수 있음

  - **Natural Language Processing(NLP)**: 컴퓨터가 인간의 언어 처리 기능을 수행할 수 있도록 하는 방법을 연구하는 분야

    - 인간의 언어와 같이 자연어로 표현된 언어를 컴퓨터가 이해할 수 있는 형태로 만드는 방법을 연구하는 학문

    - 자연어 처리의 대표적인 문제들: 문장 분류(Text Classification), 이미지 캡셔닝(Image Captioning), 기계 번역(Machine Translation), 챗봇(Chatbot) 등

    - 자연어 처리를 위해서는 RNN 구조가 많이 사용됨

    - 자연어 처리의 응용 사례

      ⇒ NMT(Neural Machine Translation)

      : 딥러닝 기법을 이용해 중국어 문장을 영어 문장으로 번역

      ⇒ Google Duplex

      → 구글의 최신 NLP 연구 결과를 토대로 인간과 대화를 자연스럽게 수행할 수 있는 인공지능 기술을 개발

      → 구글에서는 원하는 조건으로 예약을 수행할 수 있도록 도와주는 음성 가상 비서인 Google Duplex 라는 서비스를 공개

  - **Speech Recognition**: 컴퓨터가 인간의 음성 인식 능력을 수행할 수 있도록 하는 방법을 연구하는 분야

    - 음성 인식은 음성 데이터가 표현하는 문장이 무엇인지를 인식하는 문제

    - 소리를 글자로 바꿔준다고 하여 STT(Speech-To-Text) 라고도 불림

    - 음성 인식 서비스: 애플 Siri, 구글 Now, 마이크로소프트 Cortana

      ⇒ 음성 인식 기법을 사용해서 가상 비서를 구현

      ⇒ 음성 인식은 컴퓨터와 상호작용할 수 있는 차세대 인터페이스

      → 음성 인식을 사용할 경우 양손을 자유자재로 움직일 수 있는 상태에서 컴퓨터에 명령을 내릴 수 있기 때문

    - 음성 인식의 대표적인 응용 사례

      ⇒ 가정용 인공지능 스피커

      → Google Home, Amazon Alexa

      → 사용자의 음성을 인식하고, 사용자의 질문에 대한 적합한 응답을 수행

      ⇒ 자율주행차에 내장된 음성 인식 시스템

  - **Game**: 게임 환경을 이용해서 인공지능 기술 발전을 연구하는 분야

    - 게임 인공지능의 응용 사례

      ⇒ 최신- OpenAI Five: 강화 학습에 기반한 알고리즘을 통해 복잡한 Dota2 게임의 세계 최고 플레이어들을 상대로 승리를 거둠

      ⇒ 알파고(AlphaGo): 바둑을 플레이하는 인공지능, 2016

      ⇒ 딥블루(Deep Blue): 인공지능 바둑/체스 프로그램, 1996

    - 게임 인공지능을 구현하기 위한 대표적인 머신러닝 알고리즘: 강화 학습

    - 최근: **DQN(강화 학습과 딥러닝을 결합한 방법) 기법**이 사용되고 있음, 알파고에도 적용

    - 알파고 제작 회사인 딥마인드 사 → 스타크래프트를 플레이하는 인공지능을 연구 중에 있음

  - **Generative Model**: 학습 데이터의 분포를 학습해서 학습한 분포로부터 새로운 데이터를 생성하는 방법을 연구하는 분야

    - 2014년 GAN(Generative Adversarial Networks) 구조가 발표된 이후 딥러닝을 이용한 생성 모델 기법이 급속도로 주목받게 됨

    - 학습 데이터의 양을 늘려서 분류기의 성능을 높이는 데이터 증대(Data Augmentation) 기법은 생성 모델을 응용할 수 있는 기법

      ⇒ 데이터가 부족한 상황에서 추가적인 트레이닝 데이터를 만들 수 있음

    - 생성 모델 구조

      ⇒ BEGAN: 컴퓨터가 새로운 얼굴 데이터를 생성

    - 생성 모델 응용 사례: Deepfake

      ⇒ Deepfake 기술은 생성 모델 기술에 기반하여 원본 영상의 얼굴을 자연스럽게 다른 사람의 얼굴로 대체할 수 있음

      ⇒ 인공지능 기술의 부작용으로 Deepfake 기술에 기반한 가짜 뉴스 생성 등으로 인한 사회적 혼란이 우려되고 있음

      ⇒ 추가로 요즘 AI 목소리 같은거 유행하는 것도 포함될듯



#### ✅  머신러닝의 기본 프로세스- 가설 / 손실함수 / 최적화 정의

: 선형 회귀 모델의 가설, 손실함수, 최적화 정의

- 가설 정의

  : 학습하고자 하는 가설(문제)을 선형 함수의 형태로 표현

- 손실함수(= 비용함수) 정의

  : 성능 측정- 가설로 정의된 선형 함수의 파라미터 값이 적합한 값인가를 측정

  - 대표적인 손실함수: 평균제곱오차(MSE)

- 최적화 정의

  : 손실함수를 최소화하는 방향으로 파라미터값을 조정

  - 대표적인 최적화 기법: 경사하강법(Gradient Descent)

    : 다음 스텝의 파라미터 값 = 현재 스텝의 파라미터 값 - ( 러닝레이트 X 손실함수의 미분 값)

  - 러닝레이트 값을 지정하는 방법

    (원론적) 1. 상황에 맞게 설정

    (실용적) 2. 논문에 있는 러닝레이트 값과 동일한 값을 사용

    (실용적) 3. 러닝레이트 값으로 인해 파라미터 값이 발산하는 형태를 띄면 → 1/2 로 러닝레이트 값 조정



#### ✅  Batch Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent

: 최적화 정의의 3가지 기법

- Batch Gradient Descent

  : 경사하강법으로 파라미터를 한 번 업데이트할 때 전체 트레이닝 데이터를 하나의 Batch 로 적용

  - 장점: 튀는 데이터로 인해 파라미터 값의 최적화에 방해가 되는 경우를 고려할 필요가 없음
  - 단점: 한 스텝 업데이트 하는데에 시간이 오래 걸림

- Stochastic Gradient Descent

  : 경사하강법으로 파라미터를 한 번 업데이트할 때 1개의 트레이닝 데이터를 하나의 Batch 로 적용

  - Batch Gradient Descent 와 반대되는 경우
  - 장점: 파라미터 업데이트의 주기가 짧아짐(자주 업데이트)
  - 단점: 튀는 데이터가 존재한다면 파라미터 값의 최적화가 진행되지 않을 수 있음(이 경우 손실함수 값이 크게 나옴)

- Mini-Batch Gradient Descent

  :  전체 트레이닝 데이터가 n 개 있을 경우 m(n개의 데이터를 일정 갯수로 쪼갬) 개의 트레이닝 데이터를 하나의 Batch 로 적용

  - Batch Gradient Descent 와 Stochastic Gradient Descent 의 절충적인 기법

  - 가장 많이 사용됨

  - 장점: 튀는 데이터로 인해 파라미터 값의 최적화에 방해가 되는 경우를 고려할 필요가 없고, 한 스텝을 업데이트 하는데에 시간이 오래 걸리지 않음

  - ex) 1000 개의 트레이닝 데이터가 있으면

    → 100 개 씩 나누어 손실함수의 미분 값을 계산 후 다음 스텝의 파라미터 값을 갱신



#### ✅  Training Data, Validation Data, Test Data & 오버피팅(Overfitting)

- 트레이닝 데이터

  : 모델의 최적의 파라미터 값을 찾는 트레이닝 과정 중 필요한 데이터

- 검증용 데이터

  : 트레이닝 과정 중간에 사용하는 테스트 데이터

  - 오버피팅

    : 모델의 표현력이 강력하여 일반적인 상황에서의 데이터를 수용하지 못하는 현상

    - 기준과 조금이라도 어긋나는 데이터는 옳은 결과라고 판단되지 않음

      ⇒ 손실함수가 커지므로 정확도가 떨어짐

    - 검증용 데이터의 에러가 커지는 시점 전에 트레이닝을 멈추어야 함

    - 딥러닝의 경우 오버피팅에 빠지기 쉬움

      ⇒ 오버피팅을 방지하기 위한 기법인 Regularization 기법을 사용함(ex: 드롭아웃)

    - 결정직선이 과도하게 꼬아진 형태를 띔

  - 언더피팅

    : 모델의 표현력이 부족하여 트레이닝 데이터도 제대로 예측하지 못하는 현상

    - 데이터가 일정 기준만 충족하여도 옳은 결과라고 판단됨

      ⇒ 정확도가 떨어짐

    - 결정직선이 거의 직선의 형태를 띔

- 테스트 데이터

  : 최적의 파라미터 값을 갖는 모델로 실제 문제를 풀거나 모델이 잘 학습됐는지 테스트를 하기 위해 필요한 데이터



#### ✅  소프트맥스 회귀(Softmax Regression) & 크로스 엔트로피(Cross-Entropy Loss Function) & One-hot Encoding & MNIST

- 소프트맥스 회귀

  : n 개의 레이블을 분류하기 위한 가장 기본적인 모델

  - 특징
    - 선형 회귀 함수의 출력 값에 softmax 함수를 적용하면 → 각각의 값들의 합이 1이 됨
    - 모델의 각 출력 값들은 레이블에 대한 확률(= 확신의 정도) 을 나타냄

- 크로스 엔트로피(Cross-Entropy) 손실 함수

  : 분류 문제에서 주로 쓰이는 손실 함수

  - H(P,Q)=− ∑ P(x)⋅log(Q(x))

    - P(x): 참 값을 One-hot Encoding 으로 변환한 값
    - Q(x): 예측 값을 One-hot Encoding 으로 변환한 값

  - 특징

    - 분류 문제에서 MSE 보다 더 학습이 잘 됨

    - MSE 의 특성처럼

      → 모델의 예측 값이 참 값과 비슷하면 작은 값을 가짐

      → 모델의 예측 값이 참 값과 다르면 큰 값을 가짐

    - 대부분의 텐서플로 코드들에서 크로스 엔트로피 손실 함수를 사용

- MNIST 데이터셋

- One-hot Encoding

  : 각각의 범주형 데이터에서 벡터의 한 인덱스만 1 값을 갖도록 구분한 것

  - 특징

    - 1 을 제외한 나머지 벡터 성분의 값은 모두 0 의 값을 갖도록 하여 Binary Value 로 표현함

    - 머신 러닝 알고리즘을 구현할 때 타겟 데이터를 One-hot Encoding 형태로 표현하는 것이 일반적

      → 위 크로스 엔트로피 손실 함수에서의 참 값과 예측 값도 One-hot Encoding 으로 변환함(세부 사항은 크로스 엔트로피 손실 함수 참조)

  - Integer Encoding

    : n 개의 범주형 데이터가 있을 때 레이블을 각각 1~n 으로 변환하여 표현하는 것

    ⇒ 문제점: 머신러닝 알고리즘이 정수 값으로부터 잘못된 경향성을 학습하게 될 수도 있음